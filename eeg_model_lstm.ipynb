{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee333f7",
   "metadata": {},
   "source": [
    "# EEG Model Training Notebook\n",
    "\n",
    "This notebook contains the model training pipeline used for EEG classification. An overview of this notebook is as follows\n",
    "\n",
    "1. Training/Testing dataset creation\n",
    "2. Auto Encoder Model\n",
    "3. LSTM Classification Model\n",
    "4. Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734c540c-eda4-48aa-b0d4-bb7fed0cda7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkat/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import some useful libraries\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# model creation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# loading bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89800e28-1ab0-43b6-a947-93bb50f141c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training/Testing dataset creation\n",
    "\n",
    "Before we create the models, we will first prepare the data by splitting it and preprocessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904791ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_feeltrace_dir = 'ALIGNED_DATA' # directory containing *.csv files\n",
    "# hyper parameters\n",
    "window_size = 200 # must be an int in milliseconds\n",
    "subject_num = 10 # which subject to choose [1-16]\n",
    "k_fold = 5 # k for k fold validation\n",
    "label_type = 'accumulator' # 'angle' or 'pos' or 'both'\n",
    "num_classes = 3 if label_type != 'both' else 9 # number of classes to discretize the labels into\n",
    "num_features = 16 # latent space features for encoder\n",
    "encoder_learning_rate = 1e-3\n",
    "classifier_learning_rate = 1e-3 # adam learning rate\n",
    "encoder_train_epochs = 30 # train encoder duration\n",
    "classifier_train_epochs = 30 # train classifier duration\n",
    "classifier_hidden = 128 # LSTM parameter, the larger the more complicated the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f840e507-92d3-4880-b246-bd4a9e21584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing dataset\n",
    "# the data is stored in an Nx66 matrix. The first column is time in milliseconds, the second is the min/max normalized feel trace ([0,1])\n",
    "# the other 64 entries are the eeg channels \n",
    "\n",
    "def load_and_split_dataset(eeg_ft_dir = 'ALIGNED_DATA', split_size=100, subject_num = 5, k=5, label_type='angle', num_classes=3):\n",
    "    # choose the subject\n",
    "    subject_data_files = glob.glob(os.path.join(eeg_ft_dir, '*.csv'))\n",
    "    # sort the files by the index given to them\n",
    "    file_name_2_index = lambda file : int(file.split('.csv')[0].split('_')[-1])\n",
    "    subject_data_files.sort() # sort alphabetically\n",
    "    subject_data_files.sort(key=file_name_2_index) # sort by index\n",
    "    eeg_ft = subject_data_files[subject_num-1]\n",
    "\n",
    "    print(f\"Chosen subject: {eeg_ft}\")\n",
    "    \n",
    "    input_label_pair = pd.read_csv(eeg_ft).values # read the Nx66 data for a single subject\n",
    "\n",
    "    x = ( input_label_pair[:,2:] - input_label_pair[:,2:].min(axis=0, keepdims=True) )\n",
    "    y = ( input_label_pair[:,2:].max(axis=0, keepdims=True) -  input_label_pair[:,2:].min(axis=0, keepdims=True) )\n",
    "    input_label_pair[:,2:] = x/y # eeg-channel-wise min/max normalization\n",
    "\n",
    "    dataset = [input_label_pair[x : x + split_size] for x in range(0, len(input_label_pair), split_size)] # split into windows here\n",
    "    if len(dataset[-1]) < split_size:\n",
    "        dataset.pop() # remove last window if it is smaller than the rest\n",
    "\n",
    "    if label_type != 'both':\n",
    "        labels = get_label(dataset, n_labels=num_classes, label_type=label_type).squeeze() # (N, 1)\n",
    "    else:\n",
    "        labels = get_combined_label(dataset, n_labels=int(np.sqrt(num_classes))).squeeze() # (N, 1)\n",
    "\n",
    "    dataset = np.vstack([np.expand_dims(x,0) for x in dataset]) # (N, window_size, 66)\n",
    "    print(f\"Time + Feel trace + Channel set shape (N, window_size, 66):  {dataset.shape}\")\n",
    "    print(f\"label set shape (N,):  {labels.shape}\")\n",
    "\n",
    "    indices = split_dataset(labels, k=k) # split data into train/test indices using kFold validation\n",
    "    return dataset, labels, indices\n",
    "\n",
    "\n",
    "def get_label(data, n_labels=3, label_type='angle'):\n",
    "    if label_type == 'angle':\n",
    "        labels = stress_2_angle(np.vstack([x[:,1].T for x in data])) # angle/slope mapped to [0,1] in a time window\n",
    "    elif label_type == 'pos':\n",
    "        labels = np.vstack([x[:,1].mean() for x in data]) # mean value within the time window\n",
    "    else:\n",
    "        labels = stress_2_accumulator(np.vstack([x[:,1].T for x in data])) # accumulator mapped to [0,1] in a time window\n",
    "        \n",
    "    label_dist = stress_2_label(labels, n_labels=n_labels)\n",
    "    return label_dist\n",
    "\n",
    "def get_combined_label(data, n_labels=3):\n",
    "    # combine the labels using an enumerated cartesian product of the two labels sets\n",
    "    angle_labels = get_label(data, n_labels=n_labels, angle=True).squeeze() # (N, 1)\n",
    "    pos_labels = get_label(data, n_labels=n_labels, angle=False).squeeze() # (N, 1)\n",
    "\n",
    "    labels = [x for x in range(n_labels)]\n",
    "    labels_dict =  {(a, b) : n_labels*a+b for a in labels for b in labels} # cartesian product\n",
    "    combined_labels = [labels_dict[(pos, angle)] for (pos, angle) in zip(pos_labels, angle_labels)]\n",
    "    return np.array(combined_labels)\n",
    "\n",
    "\n",
    "def stress_2_label(mean_stress, n_labels=5):\n",
    "    # value is in [0,1] so map to [0,labels-1] and discretize\n",
    "    return np.digitize(mean_stress * n_labels, np.arange(n_labels)) - 1\n",
    "\n",
    "def stress_2_angle(stress_windows):\n",
    "    '''\n",
    "    do a linear least squares fit in the time window\n",
    "    stress_window: (N_samples, time_window)\n",
    "    '''\n",
    "    xvals = np.arange(stress_windows.shape[-1])/1e3/60 # time in (minutes)\n",
    "    slope = np.polyfit(xvals, stress_windows.T, 1)[0] # take slope linear term # 1/s\n",
    "    angle = np.arctan(slope)/ (np.pi/2) * 0.5 + 0.5 # map to [0,1]\n",
    "    return angle\n",
    "\n",
    "def stress_2_accumulator(stress_windows):\n",
    "    '''\n",
    "    apply an integral to the time window\n",
    "    stress_window: (N_samples, time_window)\n",
    "    '''\n",
    "    max_area = stress_windows.shape[-1]\n",
    "    xvals = np.arange(stress_windows.shape[-1]) # time in (ms)\n",
    "    integral = np.trapz(stress_windows, x=xvals)\n",
    "    return integral/max_area # map to [0,1]\n",
    "\n",
    "def split_dataset(labels, k=5):\n",
    "    '''\n",
    "    split the features and labels into k groups for k fold validation\n",
    "    we use StratifiedKFold to ensure that the class distrubutions within each sample is the same as the global distrubution\n",
    "    '''\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # only labels are required for generating the split indices so we ignore it\n",
    "    temp_features = np.zeros_like(labels)\n",
    "    indices = [(train_index, test_index) for train_index, test_index in kf.split(temp_features, labels)]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29840975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen subject: ALIGNED_DATA/EEG_FT_ALIGNED_9.csv\n",
      "Time + Feel trace + Channel set shape (N, window_size, 66):  (3308, 200, 66)\n",
      "label set shape (N,):  (3308,)\n",
      "Label class bincount: [ 171 2328  809]\n"
     ]
    }
   ],
   "source": [
    "dataset, labels, indices = load_and_split_dataset(eeg_ft_dir = 'ALIGNED_DATA', split_size=window_size, subject_num = subject_num, k=k_fold, label_type=label_type, num_classes=num_classes)\n",
    "print(f\"Label class bincount: {np.bincount(labels, minlength=num_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8a39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def encoder_split(features, labels, k_train_indices):\n",
    "    '''\n",
    "    further split each test a single k group into autoencoder/lstm training \n",
    "    maintain the same global label distribution \n",
    "    '''\n",
    "    train_features = features[k_train_indices]\n",
    "    lstm_train_features, encoder_train_features, lstm_train_labels, _  = train_test_split(train_features, labels[k_train_indices], test_size=0.2, stratify=labels[k_train_indices])\n",
    "    return lstm_train_features, encoder_train_features, lstm_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc2497-b051-4889-b598-c134db0d2959",
   "metadata": {},
   "source": [
    "## Models & Loader function\n",
    "Below we define the autoencoder and lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1327ce4-5dd9-413b-a1e5-24a235fff0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, num_features=12):\n",
    "        super(autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.7),\n",
    "            nn.Linear(128, num_features))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# autoencoder model\n",
    "# input: (N, 64)\n",
    "# latent features: Z\n",
    "# encoder: (N,64) -> (N,16) -> (N, Z)\n",
    "# decoder: (N,Z) -> (N,16) -> (N, 64)\n",
    "\n",
    "class lstm_classifier(nn.Module):\n",
    "    def __init__(self, num_features=12, num_hidden=32, dropout=0.2, n_labels=5):\n",
    "        super(lstm_classifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = num_hidden\n",
    "        self.input_size = num_features\n",
    "        self.n_classes = n_labels\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(\n",
    "            input_size =  self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(128, self.n_classes))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x, (h_t, c_t) = self.lstm_1(x)\n",
    "\n",
    "        # x -> (N, seq_len, hidden_size)\n",
    "        # h -> (1, N, hidden_size)\n",
    "        x = self.classify(h_t.squeeze(0)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002f443c-09e4-4aa9-a08d-1c435377ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        'Initialization'\n",
    "        self.x = features # (N, window_size, encoding)\n",
    "        self.labels = labels # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x = torch.from_numpy(self.x[index]).float() # eeg channels, lstm\n",
    "        y = torch.from_numpy(np.array(self.labels[index])).long() # feel trace labels int value [0,n_labels]\n",
    "        return x, y\n",
    "\n",
    "class autoencoder_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        'Initialization'\n",
    "        self.x = features # (N, window_size, 64)\n",
    "        self.x = self.x.reshape(-1, 64) # (N*window_size, 64)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x = torch.from_numpy(self.x[index]).float()\n",
    "        y = x\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f08170-7926-49da-a713-0d44a466bdf3",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50feca63-58aa-4794-8c74-65564976fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(model, num_epochs, batch_size, learning_rate, train_split):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*3, eta_min=1e-8)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = autoencoder_dataset(train_split)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=8,\n",
    "                                               shuffle=True)\n",
    "    \n",
    "    train_metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # reset metrics\n",
    "        cur_train_loss = 0 # loss\n",
    "        cur_train_sim = 0 # cosine similarity\n",
    "        \n",
    "        # set to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # loop over dataset\n",
    "        for data in tqdm(train_loader):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # metrics\n",
    "            cur_train_loss += loss.detach().cpu()\n",
    "            cur_train_sim += cosine_similarity(y.detach().cpu().numpy(), y_hat.detach().cpu().numpy()).diagonal().mean()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # average metrics over loop\n",
    "        train_loop_size = len(train_loader)\n",
    "        cur_train_loss = cur_train_loss/train_loop_size\n",
    "        cur_train_sim = cur_train_sim/train_loop_size\n",
    "        \n",
    "        \n",
    "        train_metrics.append([cur_train_loss, cur_train_sim])\n",
    "        \n",
    "        # print(f'Epoch:{epoch+1},'\\\n",
    "        #       f'\\nTrain Loss:{cur_train_loss},'\\\n",
    "        #       f'\\nTrain Cosine Similarity:{cur_train_sim}')\n",
    "        \n",
    "    return train_metrics\n",
    "\n",
    "def train_classifier(model, num_epochs=5, batch_size=1, learning_rate=1e-3, features=None, labels=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_dataset = classifier_dataset(features, labels)\n",
    "    \n",
    "    # figure out class distribution to over sample less represented classes\n",
    "\n",
    "    train_labels = labels\n",
    "    \n",
    "    # get the weights of each class as 1/occurrence\n",
    "    train_class_weight = np.bincount(train_labels, minlength=num_classes)\n",
    "    print(f\"Train label distribution: {train_class_weight}\")\n",
    "    train_class_weight = 1/train_class_weight\n",
    "    \n",
    "    # get the per sample weight, which is the likelihood os sampling\n",
    "    train_sample_weights = [train_class_weight[x] for x in train_labels]\n",
    "    \n",
    "    # sampler, weighted by the inverse of the occurrence\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(train_sample_weights, len(train_sample_weights), replacement=True)\n",
    "    \n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=8,\n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "    train_metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # reset metrics\n",
    "        cur_train_acc = 0 # accuracy\n",
    "        cur_train_pc = 0 # precision\n",
    "        cur_train_rc = 0 # recall\n",
    "        cur_train_f1 = 0 # f1\n",
    "        cur_train_loss = 0 # loss\n",
    "        \n",
    "        # set to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # loop over dataset\n",
    "        for data in tqdm(train_loader):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat_np = F.softmax(y_hat.detach(), dim=1).argmax(axis=1).cpu().numpy().squeeze().reshape(-1,) # predictions\n",
    "            y_np = y.detach().cpu().numpy().squeeze().reshape(-1,) # labels\n",
    "            \n",
    "            # metrics\n",
    "            prf = precision_recall_fscore_support(y_np, y_hat_np, average='macro', zero_division=0)\n",
    "            \n",
    "            cur_train_acc += np.mean(y_hat_np == y_np)\n",
    "            cur_train_pc += prf[0]\n",
    "            cur_train_rc += prf[1]\n",
    "            cur_train_f1 += prf[2]\n",
    "            cur_train_loss += loss.detach().cpu()\n",
    "        \n",
    "        # average metrics over loop\n",
    "        train_loop_size = len(train_loader)\n",
    "        cur_train_acc  = cur_train_acc/train_loop_size\n",
    "        cur_train_pc   = cur_train_pc/train_loop_size\n",
    "        cur_train_rc   = cur_train_rc/train_loop_size\n",
    "        cur_train_f1   = cur_train_f1/train_loop_size\n",
    "        cur_train_loss = cur_train_loss/train_loop_size\n",
    "        \n",
    "        \n",
    "        train_metrics.append([cur_train_acc, cur_train_pc, cur_train_rc, cur_train_f1, cur_train_loss])\n",
    "            \n",
    "        # print(f'Epoch:{epoch+1},'\\\n",
    "        #       f'\\nTrain Loss:{cur_train_loss},'\\\n",
    "        #       f'\\nTrain Accuracy:{cur_train_acc},'\\\n",
    "        #       f'\\nTrain Recall: {cur_train_rc},'\\\n",
    "        #       f'\\nTrain precision: {cur_train_pc},' \\\n",
    "        #       f'\\nTrain F1-Score:{cur_train_f1},')\n",
    "        \n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a929a99-df1c-43f0-aecd-645c6babadd1",
   "metadata": {},
   "source": [
    "#### Run Training for encoder and classifier (with encoding) and test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a2229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training k=0\n",
      "Training Encoder!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:10<00:00,  4.80it/s]\n",
      "100%|██████████| 52/52 [00:11<00:00,  4.54it/s]\n",
      " 17%|█▋        | 9/52 [00:02<00:11,  3.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_640163/3812654586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Encoder!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mencoder_train_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_train_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_train_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mencoded_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_classifier_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_train_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_640163/447098082.py\u001b[0m in \u001b[0;36mtrain_encoder\u001b[0;34m(model, num_epochs, batch_size, learning_rate, train_split)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mcur_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mcur_train_sim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36misspmatrix\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m     \"\"\"Is x of a sparse matrix type?\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def encode_classifier_data(encoder_model, classifier_features):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder_model.eval()\n",
    "    with torch.no_grad():\n",
    "        prev_shape = classifier_features.shape # (N, window_size, 64)\n",
    "        x = torch.from_numpy(classifier_features).float().reshape(-1,1,64).to(device) # encode the 64 channels\n",
    "        x_encoded = encoder_model.encode(x).reshape(prev_shape[0], prev_shape[1], num_features).detach().cpu().numpy()\n",
    "        return x_encoded\n",
    "        \n",
    "k_acc = [] # accuracies for each fold\n",
    "k_f1 = [] # f1 score for each fold\n",
    "\n",
    "for cur_k in range(len(indices)):\n",
    "    print(f\"Training k={cur_k}\")\n",
    "    train_index, test_index = indices[cur_k]\n",
    "\n",
    "    lstm_train_features, encoder_train_features, lstm_train_labels = encoder_split(dataset, labels, train_index)\n",
    "\n",
    "\n",
    "    encoder_model = autoencoder(num_features=num_features)\n",
    "    classifier_model = lstm_classifier(num_features=num_features, num_hidden=classifier_hidden, dropout=0.5, n_labels=num_classes)\n",
    "\n",
    "    print('Training Encoder!')\n",
    "    encoder_train_metrics = train_encoder(encoder_model, encoder_train_epochs, batch_size=2048, learning_rate=encoder_learning_rate, train_split=encoder_train_features[:,:,2:])\n",
    "\n",
    "    encoded_train_features = encode_classifier_data(encoder_model, lstm_train_features[:,:,2:])\n",
    "    print('Training Classifier!')\n",
    "    classifier_train_metrics = train_classifier(classifier_model, classifier_train_epochs, batch_size=64, learning_rate=classifier_learning_rate, features=encoded_train_features, labels=lstm_train_labels)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lstm_test_features, lstm_test_labels =  dataset[test_index], labels[test_index]\n",
    "    print(f\"Test label distribution: {np.bincount(lstm_test_labels, minlength=num_classes)}\")\n",
    "    encoded_test_features = encode_classifier_data(encoder_model, lstm_test_features[:,:,2:])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        classifier_model.eval()\n",
    "        x_encoded  = torch.from_numpy(encoded_test_features).float().to(device)\n",
    "        y = lstm_test_labels\n",
    "        y_hat = classifier_model(x_encoded)\n",
    "        y_hat = F.softmax(y_hat.detach(), dim=-1).cpu().numpy()\n",
    "        preds = y_hat\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(20,20), dpi=120)\n",
    "    axs.set_title(f\"LSTM Feel Trace Model Confusion Matrix - Emotion-as-{label_type}\", fontsize=20)\n",
    "    axs.set_xlabel(\"Predicted Label\", fontsize=15)\n",
    "    axs.set_ylabel(\"True Label\", fontsize=15)\n",
    "\n",
    "\n",
    "    prf = precision_recall_fscore_support(lstm_test_labels, np.array([x.argmax() for x in preds]), average='macro', zero_division=0)\n",
    "    acc = np.mean(lstm_test_labels == np.array([x.argmax() for x in preds]))\n",
    "    print(f\"Precision: {prf[0]}\")\n",
    "    print(f\"Recall: {prf[1]}\")\n",
    "    print(f\"F1-Score: {prf[2]}\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    k_acc.append(acc)\n",
    "    k_f1.append(prf[2])\n",
    "\n",
    "    cm = confusion_matrix(lstm_test_labels, [x.argmax() for x in preds], labels=np.arange(num_classes), normalize=None)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))\n",
    "\n",
    "    disp.plot(ax=axs)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Accuracy, Average accuracy: {k_acc}, {np.mean(k_acc)}\")\n",
    "print(f\"F1-Score, Average F1-Score: {k_f1}, {np.mean(k_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adabc22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
